---
layout: post
title: "[논문 리뷰] Spikformer : When Spiking Neural Network Meets Transformer"
date: 2025-03-11 00:00:00 +0900
categories: 
  - tech
  - paperreview  
---

* toc
{:toc}

[논문 링크](https://arxiv.org/abs/2209.15425)

Spikformer는 ViT의 변형 모델로, 활성 함수로 spike neuron을 넣은 모델. 
SNN의 장점들을 유지하며 트랜스포머에 SNN을 이식했다.

## Abstract
---
- self-attention과 spiking self attention(SSA)을 동시에 활용하는 spikformer 모델을 제시
- spike form이 0과 1로만 이뤄져있기 때문에, sofrmax도, 곱연산도 없어 연산 부담이 적음
- SSA를 적용한 Spikformer(66.3M)는 image classification task에서 다른 SNNs와 비교해 sota 달성


## Intro
---
#### SNN
- 1세대 perceptron, 2세대 ANN, 3세대 SNN
- Power consumption, bio-plausibility 등등 측면에서 promising
- ResNet-like, Spiking RNN, Spiking graph neural network 등등에 이식됨

#### Spiking Self attention 
- Self-attention이 human hippocampus system과 유사함이 밝혀져 있음 (Whittington et al., 2022)
- 이 연구에서는 self-attention에 SNN을 적용하면서, 새로운 SSA(spike self-attention) 매커니즘 제시
- input, output이 모두 spike form (0 or 1)이고, negative값이 없으므로 아예 softmax를 없앰
- logical AND연산과 ADD 연산으로만 구현됨
- CosFormer, Performer와 같은 다른 transformer 변형 모델에서도 softmax를 제거한 바 있음

#### Contribution
- SSA(Spiking self attention) 매커니즘 제시
- SSA를 적용한 Spikformer 모델 제시
- Static, neuromorphic dataset에서 sota 달성, ImageNet에서 74% 달성.


## Related Work
---
#### Vison Transformer
- Image를 patch로 잘라 linear projection하여 token처럼 취급하는 모델

#### LIF(Leaky Integrate-and-Fire) dynamics
2가지 방법으로 Deep SNN 모델을 얻을 수 있다.  
- ANN-SNN Conversion : pretrained model의 ReLU를 spiking neuron으로 바꾸기
- Direct Training (처음부터 학습) : spike는 미분 불가능하므로 backprop시 surrogate gradient 사용해야 함

LIF spike neuron 모델은 SNN의 기본 unit으로, 본 연구에서는 activation function으로 LIF를 사용한다.


$$
\displaylines{
H[t] = V[t-1] + \frac{1}{τ} \cdot (X[t] - (V[t-1] - V_{reset})) \\
S[t] = Θ(H[t] - V_{th}) \\
V[t] = H[t] \cdot (1-S[t]) + V_{reset} \cdot S[t]
}
$$

- H\[t] : membrane potential, 이전 time step의 V\[t-1]값에다 순수 input current의 평균.
- S\[t] : t시점의 spike. 헤비사이드 스텝 함수에 의해, $$V_{th}$$ 값보다 membrane potential 값이 크면 1, 아니면 0.
- V\[t] : trigger event 후의 membrane potential. spike가 일어났으면 $$V_{reset}$$으로 초기화, 아니면 H\[t] 유지.

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/paper/8_1.gif)

즉, 특정 시간 안에 pre-synaptic neuron에서 신호 빈도가 높게 들어오면 firing하는 매커니즘이다. 
실제 코드에서는 T가 4로 설정되어 있는데, 즉 4번 연속으로 똑같은 텐서를 보내서 time window 안에서 충분히 많이 신호가 들어오면 다음으로 firing한다.
spikingjelly 패키지가 제공하는 클래스로 아래 코드와 같이 불러와 사용할 수 있다. 

```python
from spikingjelly.clock_driven.neuron import MultiStepLIFNode
``` 



## Method
---

#### Architecture

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/paper/8_2.png)

기본적으로 ViT와 비슷한데, 이미지 패치를 linear projection하는 방법과 self-attention block만 조금 변형되었다. 
전체 모델의 수식 흐름을 따라가면 아래와 같다.

1. 2D 이미지 시퀀스 $$I$$는 $$R^{T \times C \times H \times W}$$  로 표현된다. T개의 time step만 빼고는 높이와 너비, RGB 채널의 곱으로 표현되는 것은 동일하다. N개로 쪼개진 각 이미지 패치는 Spiking Patch Spliting (SPS) 모듈을 통해 D차원의 벡터로 변환된다.

$$
\displaylines{
x = SPS(I), I \in R^{T\times C\times H\times W}, x \in R^{T\times N\times D}
}
$$

2. relative positional encoding (RPE)를 구해 더해준다. 

$$
\displaylines{
RPE = SN(BN(Conv2d(x))), RPE \in R^{T\times N\times D} \\ 
X_{0} = x + RPE, X_{0} \in R^{T\times N\times D}
}
$$

3. spiking self attention 레이어와 MLP 레이어를 통과시킨다. 각각 residual connection이 있다.

$$
\displaylines{
X_{l}^{'} = SSA(X_{l-1}) + X_{l-1}, X_{l}^{'} \in R^{T\times N\times D} \\
X_{l} = SSA(X_{l}^{'}) + X_{l}^{'}, X_{l} \in R^{T\times N\times D} 
}
$$

4. Global average pooling과 classification head를 거쳐 최종 prediction Y를 구한다.

$$
Y = CH(GAP(X_{L}))
$$

#### Spiking Patch Spliting (SPS)
- conv2d와 batch norm, spiking, max pooling으로 구성된다.
- 각 패치를 D차원의 spike-form feature로 변환한다.

#### Spking Self Attention (SSA)
vanilla self attention과 계산 방법이 조금 다르다.

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/paper/8_3.png)

1) 먼저 input X로부터 각각 행렬곱 -> Batch Norm -> LIF를 통해 Q,K,V matrix를 얻는다.
$$
Q = SN_{Q}(BN(X\cdot W_{Q})) \\
K = SN_{K}(BN(X\cdot W_{K})) \\
V = SN_{V}(BN(X\cdot W_{V})) \\
$$
2) 이를 그대로 곱한 후 scaling factor s로 크기를 조절해준다.
$$
SSA^{'}(Q,K,V) = SN(Q\cdot K^{T}\cdot V \times s)
$$
3) 마지막으로 Linear -> BatchNorm -> LIF 를 통과시킨다.
$$
SSA^{'}(Q,K,V) = SN(BN(Linear(SSA^{'}(Q,K,V))))
$$

spike sequence Q,K,V는 모두 0,1값이기 때문에 단순히 AND와 addition 연산만이 수행된다. 또, 행렬간 연산 순서가 interchangable해, N의 크기나 D의 크기에 따라서 시간복잡도를 조절할 수 있다는 장점도 있다.

결국 이 모델이 각각의 $W_{Q}, W_{K}, W_{V}$ 가중치에 반영하는 내용들은, **attention할 신호들을 얼마나 frequent하게 줄 것인가?** 가 된다.


## Experiments
---
CIFAR, ImageNet 데이터셋으로 image classification을 실험했다. 
- pytorch, spikingjelly, timm 사용

비교 실험의 결론은 다음과 같다.
- 다른 SNN 모델들보다 가장 성능이 좋음
- Transformer (Spikformer 아키텍쳐에서 어텐션만 vanilla로 바꾼 모델)와 성능 유사

Ablation Study의 주요 결론은 다음과 같다.
- encoder block의 개수가 늘어날수록 성능 증가
- embedding dim이 늘어날수록 성능 증가
- time step이 늘어날수록 성능 증가


![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/paper/8_4.png)

마지막 encoder block의 4번째 time step에서 추출한 attention map이다. attention map이 0과 1로만(spike form) 이뤄져있기 때문에 모델이 어디에 집중하고 있는지 binary하게 나타낼 수 있다. 


## Insights
---
- embedding enrichment라는 트랜스포머의 중요 특성을 잃어버린다.
- 성능 비교 대상인 Transformer 모델은 ViT가 아니라, SSA를 위해 설계된 Spikformer 아키텍쳐에서 Vanilla self attention만 적용한 커스텀 모델이다. ViT는 훨씬 성능이 높게 나온다. 
- 저전력/저비용 모델이라는 차원에서 기술적인 의미가 있어 보인다.
- Neuroscience 측면에서 보면 SNN을 Transformer에 적용했다는 의의가 있을 듯 하다. 이 이후에도 이 논문의 아이디어를 배경으로 spike-driven Transformer, Spikformer V2 등등 여러 transformer 구현체들이 많이 있다.