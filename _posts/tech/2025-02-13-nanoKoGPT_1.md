---
layout: post
title: "GPT 기초부터 만들기 by Andrej Karpathy (1) - Tokenization, Dataloader"
date: 2025-02-13 00:00:00 +0900
categories:
  - tech
---

<iframe title="Let's build GPT: from scratch, in code, spelled out." src="https://www.youtube.com/embed/kCc8FmEb1nY?feature=oembed" height="113" width="200" style="aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;" allowfullscreen="" allow="fullscreen"></iframe>

<br/>
GPT의 아키텍쳐를 기초부터 차근차근 만들어보자. character 단위로 학습하는 모델을 만들어본다. 데이터는 all of shakesphere를 사용하지만, 여기서는 한국어 데이터셋을 사용하도록 약간씩 코드를 변형해볼 것이다. 

[Google Colab](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEVMTzFyV2FoT1QtckNKZzVtRThUdTVGOGw4UXxBQ3Jtc0ttaGZBbEx3Z1lhRWhoZzZ4NGtMeDFXejBjVjRjRF9XTVk1eGRSZUJGNlpEZTRtN0M1VjBfb3VXN29nZm42eTNDSnpOSWlnUXBJdEl2WUdDZjNhbTItWWszMm9TZEZJUktob0JmT0tCb3hCMVlFaXR3dw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-%3Fusp%3Dsharing&v=kCc8FmEb1nY)
[NanoGPT Lecture Repository](https://github.com/karpathy/ng-video-lecture)

<br/>
## Tokenization
---

텍스트 데이터로부터 `set()` 함수를 사용해 `vocab_list`를 만든다. 여기서는 BPE나 WordPiece, SentencePiece와 같은 토큰화 방법을 쓰지 않고 순수 character 기반으로만 vocab을 구성한다. Karpathy의 세익스피어 데이터셋(`input.txt`)에서는 알파벳 26개, 대소문자에 특수문자까지 해서 vocab size가 65이다. 

```python
# 파일을 불러오기
with open("input.txt", "r", encoding="utf-8") as f :
    text = f.read()

# 중복 제거, 정렬
chars = sorted(list(set(text)))

# character 단위 양방향 변환 딕셔너리
stoi = {ch:i for i,ch in enumerate(chars)}
itos = {i:ch for i,ch in enumerate(chars)}

# encode / decode 함수 선언
def encode(s : str) -> list :
    global stoi
    return [stoi[c] for c in s]

def decode(l : list) -> str :
    global itos
    ''.join([itos[i] for i in l])
```

하드코딩된 부분들을 정리하고, 편리하게 이용할 수 있도록 함수화한다.
<br/>

```python
class KorCharTokenizer() :
    def __init__(self, path):
        self.path = path
        self.stoi = {}
        self.itos = {}
        self.special_chars = " 0123456789~!@#$%^&*()[]{}:;,./<>?/*-+=_`"
        self.build_vocab()

    def get_vocab_size(self) -> int :
        return len(self.stoi)

    def build_vocab(self) -> None :
        # 텍스트 추출
        with open(self.path, "r", encoding="cp949") as f :
            text = f.read()
            
        # 특수문자 추가
        text += self.special_chars

        # 중복 제거, 정렬
        chars = sorted(list(set(text)))

        # character 단위 양방향 변환 딕셔너리 구축
        self.stoi = {ch:i for i,ch in enumerate(chars)}
        self.itos = {i:ch for i,ch in enumerate(chars)}  

    # encode 함수
    def encode(self, s : str) -> list :
        return [self.stoi[c] for c in s]

    # decode 함수
    def decode(self, l : list) -> str :
        return ''.join([self.itos[i] for i in l])
    
```

> 한국어는 자음x모음x받침까지 훨씬 많은 조합을 만들 수 있다. 받침이 없는 경우 399개 (자음 19개 x 모음 21개), 받침이 있는 경우 10,773개 (자음 19개 x 모음 21개 x 받침 27개). GPT2가 사용하는 vocab size가 50,000개정도인 걸 감안하면 정말 많은 양이다. 한국어는 단순 character 단위의 토큰으로 학습을 시킨다고 하더라도 영어보다 훨씬 많은 양의 데이터가 필요할 것이다.

토크나이저 + 인코더로 1차원의 정수 리스트를 만들어 torch로 텐서화해 둔다. 리스트로 변환하고자 할 때는 텐서에 `.tolist()` 메소드를 붙여 사용하면 된다.



<br/>
## Dataset Split
---
원하는 비율만큼 이 텐서를 잘라 일부는 `train_data`로, 일부는 `val_data`로 사용한다. 이건 데이터로더나 다른 다양한 split 관련 함수로도 할 수 있지만, 여기서는 직접 인덱싱해 나눈다. 

```python
def train_val_split(data, ratio) :
    n = int(ratio*len(data))
    train_data = data[:n]
    val_data = data[n:]
    return train_data, val_data
```

함수로 래핑해두었다. 



<br/>
## DataLoader, Batch
---
실제 데이터가 트랜스포머에서 학습될 때는 chunk 단위로 잘려서 모델에 들어간다. 이 길이는 모델에 feeding할 수 있는 최대 토큰의 개수이며, `context length`라고 부른다. GPT-1의 `context length`는 512개였으며, GPT-2에서는 1024개, GPT-3에서는 2048개, GPT-4는 128,000개였다. 

GPT는 CLM(Causal Language Model) 방식으로 학습하기 때문에 이렇게 잘린 chunk들을 앞에서부터 오직 단방향으로 다음 토큰을 예측하도록 학습한다.

```
When input is tensor([38]) the target : 72
When input is tensor([38, 72]) the target : 81
When input is tensor([38, 72, 81]) the target : 82
When input is tensor([38, 72, 81, 82]) the target : 83
When input is tensor([38, 72, 81, 82, 83]) the target : 1
When input is tensor([38, 72, 81, 82, 83,  1]) the target : 35
When input is tensor([38, 72, 81, 82, 83,  1, 35]) the target : 72
When input is tensor([38, 72, 81, 82, 83,  1, 35, 72]) the target : 83
```

한가지 더 유념할 사항은 `batch_size`이다. GPU 덕택에 여러 chunk가 병렬적으로 연산/학습된다. 여러 chunk를 랜덤한 위치에서 골라 stack해주는 `get_batch` 함수를 만들어 `x_batch`와 `y_batch`를 가져온다.

```python
def get_batch(split, train_data, val_data, context_length, batch_size) :
    if split == "train" :
        data = train_data
    else :
        data = val_data

    # batch_size 개의 인덱스 위치를 랜덤하게 잡고
    ix = torch.randint(len(data)-context_length, (batch_size, ))

    # 그 위치에서 context_length만큼 문장을 파싱해온다.
    x_batch = torch.stack([data[i : i+context_length] for i in ix])
    y_batch = torch.stack([data[i+1 : i+context_length+1] for i in ix])
    return x_batch, y_batch
```


여기서는 토큰들이 겹칠 수 있도록 context_length만큼 샘플링되어 문제가 없지만, 만일 데이터들이 문장 단위로 잘려있다면 batch화하는 과정에서 별도의 작업들이 더 필요하다. 토큰의 개수가 각기 다른 문장들을 같은 context_length 안에 맞춰야 하기 때문이다. 만일 context_length가 20이어서 토큰 길이가 각각 \[12,31,20,11,19]인 batch를  \[20,20,20,20,20]으로 맞춰야 한다면 문장에 따라 늘어난 부분을 자르는 truncation이 필요하고 다른 하나는 빈 부분을 디폴트 토큰으로 채우는 collating이이 필요하다. 보통 Dataloader에서 이 작업들을 해준다. 
