---
layout: post
title: "GPT 기초부터 만들기 by Andrej Karpathy (4) - FFN, AddNorm"
date: 2025-02-18 00:00:00 +0900
categories:
  - tech
  - pytorch
related_posts:
  - _posts/tech/pytorch/2025-02-13-nanoKoGPT_1.md
  - _posts/tech/pytorch/2025-02-14-nanoKoGPT_2.md
  - _posts/tech/pytorch/2025-02-18-nanoKoGPT_3.md
---

<iframe title="Let's build GPT: from scratch, in code, spelled out." src="https://www.youtube.com/embed/kCc8FmEb1nY?start=5065" height="113" width="200" style="aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;" allowfullscreen="" allow="fullscreen"></iframe>

* toc
{:toc}


<br/>
## FFN 
---

Self Attention Block에서는 Attention score를 추출해 embedding vector가 더 풍부한 정보를 포함하도록 만들었다. 그 다음으로는 잘 정제한 재료들을 통해 모델이 유의미한 결론을 도출하도록 해야 한다. FeedForward network는 그렇게 만든 embedding vector들에 대해 모델이 생각하게 하는 블록이다.

FeedForward 클래스를 만들어 코드에 적용해보자. 받은 차원을 그대로 다시 출력한다.

```python
class FeedForward(nn.Module) :
    def __init__(self, inout_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(inout_size, inout_size),
            nn.ReLU(),
        )
    def forward(self, x) :
        return self.net(x) # (Batch size, context_length, head_size)
```

이제 해당 레이어를 `AttentionLanguageModel` 의 객체로 만들어 `forward()` 함수에 통과시킨다.

```python
class AttentionLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size, context_length, head_size, num_heads):
	    ...
	    self.ffn = FeedForward(self.head_size)
	    ...

    def forward(self, idx, targets=None) :
        B,T = idx.shape
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) 
        x = tok_emb + pos_emb

        x = self.self_attention_head(x)
        x = self.ffn(x)
        logits = self.lm_head(x)
        ...
```



FFN을 추가하여 학습한 결과. 
```
수위 엄마랑 젖삭일어났던 찰나 머구 만나......"


"..-_-..."


역시 들었어...뭘..."


"응..얘들아 우리 네다 온 도경 한 척 하고

몽롱 병원0한다.

대체 어쩔 수 있잖아...

까맣게 하고 있으니까..


너머에서 터져 왔다고 웃어차피 시작도 할 수도록

생각하게 나타게 얼마나 느꼈던 여학생들에서 다음을
```

주변의 character들의 벡터들을 함께 고려하여 얻은 enriched embedding vector를 사용해 FFN 네트워크를 통과시키자 훨씬 모양새가 좋아진다. 단순히 말이 되지 않을 뿐, 점점 제대로 된 모양새를 갖추어 간다.



<br/>
## 블록화
---
이제 Self Attention Block + FFN 으로 이뤄진 unit을 하나의 Block으로 묶어서 연속적으로 쌓아보자. 각 Decoder들은 주변의 맥락에 맞게 embedding vector들을 enrich하도록 학습되고, Decoder를 통과할수록 embedding vector들이 더 유의미해질 것이다. 

코드로 구현할 때는 `MultiHeadAttention` 객체와 `FeedForward` 객체를 순차적으로 통과하는 `DecoderBlock` 클래스를 선언한다.

```python
class DecoderBlock(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.self_attention_block = MultiHeadAttention(context_length, embedding_size, head_size, num_heads)
        self.ffn = FeedForward(head_size, embedding_size)

    def forward(self, x) :
        
        # (Batch size, context_length, head_size) -> (Batch size, context_length, head_size)
        x = self.self_attention_block(x)
        
        # (Batch size, context_length, head_size) -> (Batch size, context_length, embedding_size)
        x = self.ffn(x)

        return x
```

이를 모델에 `nn.Sequential()`로 쌓아올리고, `forward()`로 통과시킨다.
```python
class AttentionLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size, context_length, head_size, num_heads):
        super().__init__()
        self.token_embedding_table = nn.Embedding(
            num_embeddings = vocab_size,
            embedding_dim = embedding_size,
            )

        self.position_embedding_table = nn.Embedding(
            num_embeddings = context_length,
            embedding_dim = embedding_size,
            )

        self.decoder_blocks = nn.Sequential(
            DecoderBlock(context_length, embedding_size, head_size, num_heads),
            DecoderBlock(context_length, embedding_size, head_size, num_heads),
            DecoderBlock(context_length, embedding_size, head_size, num_heads), 
        )

        self.lm_head = nn.Linear(embedding_size, vocab_size)

    def forward(self, idx, targets=None) :
        B,T = idx.shape

        # (Batch size, context_length) -> (Batch size, context_length, embedding_size)
        tok_emb = self.token_embedding_table(idx)

        # (context_length, embedding_size)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))

        # (Batch size, context_length, embedding_size), pos_emb에서 broadcasting 발생
        x = tok_emb + pos_emb

        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, embedding_size)
        x = self.decoder_blocks(x)

        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, vocab_size)
        logits = self.lm_head(x)
        
        ...
```

그런데 이렇게 신경망을 깊게 쌓기 시작하면 Gradient Vanishing 문제가 발생하여 학습이 잘 되지 않는다. 이를 해결하는데는 두 가지 방식이 있다. 

<br/>
## Skip Connection (Add) 구현
---

첫 번째는 Residual Connection 또는 Skip Connection이라고 부르는 Add 연산이다. 연산된 값에 원본 x값을 추가해준다는 관점으로도 볼 수 있지만, 반대로 바라볼 수도 있다. **원본의 embedding vector들은 계속 다음 block으로 흐르되, self-attention block이나 ffn block으로 fork되어서 연산한 결과를 원본에 더해줌으로써 embedding vector들을 enrich**해준다는 관점에서 바라볼 수 있다. 

따라서 결과값들을 계속 x에 누산하는 식으로 구현해주면 된다. 다음 그림의 개념을 그대로 구현한다.

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/nanoKoGPT/4_structure.webp)

> 엄밀하게 말하면 여기에서는 cross-attention이 없으므로, Masked Multi-Head Attention과 FFN만 구현된 상태라고 볼 수 있다.


`DecoderBlock` 클래스에서 self attention layer와 ffn layer를 통과할 때 각각 원본 x의 텐서에 누산되도록 한다.
```python
class DecoderBlock(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.self_attention_block = MultiHeadAttention(context_length, embedding_size, head_size, num_heads)
        self.ffn = FeedForward(head_size, embedding_size)

    def forward(self, x) :
        # (Batch size, context_length, head_size) -> (Batch size, context_length, head_size)
        x = x + self.self_attention_block(x)
        # (Batch size, context_length, head_size) -> (Batch size, context_length, embedding_size)
        x = x + self.ffn(x)
        return x
```

> 여기서 x += self.ffn(x) 으로 할당 연산자를 줄여서 쓰면 오류가 발생한다. 병렬처리될 수 있는 텐서는 inplace로 연산하면 race condition으로 오류가 발생할 수 있다. x = x + self.ffn(x) 로 풀어서 쓰자.

다만 이렇게 할 때는 차원 수가 올바르게 맞아야 한다. 원본 x의 차원 상태인 (batch_size, context_length, embedding_size) 로 되돌려주는 projection layer를 self-attention layer와 ffn layer에 하나씩 추가하자. 

```python
class MultiHeadAttention(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.head_size = head_size
        self.heads = nn.ModuleList([Head(context_length, embedding_size, head_size//num_heads) for _ in range(num_heads)])
        self.proj = nn.Linear(head_size, embedding_size)

    def forward(self, x) :
        # (Batch_size, context_length, head_size) -> (Batch_size, context_length, head_size)
        x = torch.cat([h(x) for h in self.heads], dim=-1)
        # (Batch_size, context_length, head_size) -> (Batch_size, context_length, embedding_size)
        x = self.proj(x)
        return x
```

```python
class FeedForward(nn.Module) :
    def __init__(self, in_size, out_size):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_size, 4*out_size),
            nn.ReLU(),
            nn.Linear(4*out_size, out_size),
        )

    def forward(self, x) :
        return self.net(x) # (Batch size, context_length, embedding_size)
```

<br/>
## Batch Normalization
---
두 번재는 Batch Normalization 연산이다. 투입되는 벡터의 값들을 ~$$N(0,1)$$로 교정해주어 값의 분포나 평균이 극단적으로 치우치는 것을 방지하여 학습을 안정화시키는 방법이다. 

논문에서는 각 block을 통과한 후에 Add, Norm 연산을 수행하도록 되어있지만, 현재의 구현 방식은 각 block에 투입하기 전에 Norm을 적용한다. `nn.LayerNorm()` 각 Block과 마지막 `vocab_size` dimension으로 변환하기 전에 추가해준다.

```python
class DecoderBlock(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.self_attention_block = MultiHeadAttention(context_length, embedding_size, head_size, num_heads)
        self.ffn = FeedForward(embedding_size, embedding_size)
        self.ln1 = nn.LayerNorm(head_size)
        self.ln2 = nn.LayerNorm(embedding_size) 

    def forward(self, x) :

        # (Batch size, context_length, head_size) -> (Batch size, context_length, embedding_size)
        x = x + self.self_attention_block(self.ln1(x))

        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, embedding_size)
        x = x + self.ffn(self.ln2(x))

        return x
```

```python
self.decoder_blocks = nn.Sequential(
            DecoderBlock(context_length, embedding_size, head_size, num_heads),
            DecoderBlock(context_length, embedding_size, head_size, num_heads),
            DecoderBlock(context_length, embedding_size, head_size, num_heads),
            nn.LayerNorm(embedding_size),
        )
```


3개의 DecoderBlock을 쌓은 상태로 학습을 진행해보자.

```
흥분한 아한 거 끝으로,

아련이 지금 이 순간 기다리를 헤집 생긴 채

...

.....



뒤늦은 멍이 되어지는 내 손바닥 쪽으로

거울 속에 내밀며 가만히 날 팽팽 치다가

본 이 꼴통들 다 큰 아주 상주에 제일 싫어하는 거야..

..뭐야.."


그 안에서 약봉껄여 어찌 두었던 건지 같아..

도경이 누구보다 들어가기 시작
```

Final Loss가 1.18로 큰 폭으로 떨어졌다. 모든 부분이 동시에 말이 되지는 않지만 읽을 수 있는 chunk가 훨씬 늘어났다.



<br/>
## 모델을 키우기
---
#### dropout의 추가
각 residual connection 전에 넣어준다. 즉 forward() 메소드의 최종 return 전에 들어간다. 각 Head에, FFN에, Block에 추가해주자.

#### 최종 파라미터 설정
이제 모델을 모두 구축했다. 원하는대로 파라미터 설정을 맞추고 학습을 돌려보자. NVIDIA GTX 1660으로 약 1시간 정도 소요된다.
```
context_length = 256
embedding_size = 256
head_size = 128
num_heads = 8
num_blocks = 6

batch_size = 64
epoch = 10000
lr = 2e-4

max_new_tokens = 200
```


생성 결과는 아래와 같다. Final Loss : 1.333
```
 눈앞에 쉿진 얼굴로 두 그 눈 앞에 마치 채

신해준 일뿐인 갑게 따라 건넸다.

..

"야아니......."

..

....외면하아 소녀의 잔을 파악한 바꾸에

땅바닥 위로 떨어져 버리는 채원우의 차 숨소리.


채.나 할말이 없었어..

비행기 좀 멈춰줘 둬 두었으니..


아무데도 말하지만

아무것도
```


최종 코드.
```python
import torch
import torch.nn as nn
from torch.nn import functional as F
from tqdm import tqdm
import argparse
  

class KorCharTokenizer() :
    def __init__(self, path=""):
        self.path = path
        self.stoi = {}
        self.itos = {}
        self.all_kor_chars = "./all_kor_chars.txt"
        self.special_chars = " 0123456789~!@#$%^&*()[]{}:;,./<>?/*-+=_`"
        self._build_vocab()

    def _build_vocab(self) :
        # 텍스트 추출 (경로 주어지지 않은 경우 모든 한국어 음소)
        if self.path == "" :
            self.path = self.all_kor_chars
        with open(self.path, "r", encoding="cp949") as f :
            text = f.read()
        # 특수문자 추가
        text += self.special_chars
        # 중복 제거, 정렬
        chars = sorted(list(set(text)))
        # character 단위 양방향 변환 딕셔너리
        self.stoi = {ch:i for i,ch in enumerate(chars)}
        self.itos = {i:ch for i,ch in enumerate(chars)}

    def __len__(self) :
        return len(self.stoi)

    def get_vocab_size(self) -> int :
        return len(self.stoi)
    
    # encode 함수
    def encode(self, s : str) -> list :
        return [self.stoi[c] for c in s]

    def decode(self, l : list) -> str :
        return ''.join([self.itos[i] for i in l])        
  
def get_batch(split, train_data, val_data, context_length, batch_size) :
    if split == "train" :
        data = train_data
    else :
        data = val_data
    # batch_size 개의 인덱스 위치를 랜덤하게 잡고,
    ix = torch.randint(len(data)-context_length, (batch_size, ))
    # 그 위치에서 context_length만큼 문장을 파싱해온다.
    x = torch.stack([data[i : i+context_length] for i in ix])
    y = torch.stack([data[i+1 : i+context_length+1] for i in ix]) # y는 같은 인덱스에서 한 칸 다음 토큰을 보여준다.
    return x,y

class DecoderBlock(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.self_attention_block = MultiHeadAttention(context_length, embedding_size, head_size, num_heads)
        self.ffn = FeedForward(embedding_size, embedding_size)
        self.ln1 = nn.LayerNorm(embedding_size)
        self.ln2 = nn.LayerNorm(embedding_size)

    def forward(self, x) :
        # (Batch size, context_length, head_size) -> (Batch size, context_length, embedding_size)
        x = x + self.self_attention_block(self.ln1(x))
        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, embedding_size)
        x = x + self.ffn(self.ln2(x))
        return x

class FeedForward(nn.Module) :
    def __init__(self, in_size, out_size, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_size, 4*out_size),
            nn.ReLU(),
            nn.Linear(4*out_size, out_size),
            nn.Dropout(dropout),
        )
    def forward(self, x) :
        return self.net(x) # (Batch size, context_length, embedding_size)

class MultiHeadAttention(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads, dropout=0.1):
        super().__init__()
        self.head_size = head_size
        self.heads = nn.ModuleList([Head(context_length, embedding_size, head_size//num_heads) for _ in range(num_heads)])
        self.proj = nn.Linear(head_size, embedding_size)
        self.dropout = nn.Dropout(dropout)  

    def forward(self, x) :
        # (Batch_size, context_length, head_size) -> (Batch_size, context_length, head_size)
        x = torch.cat([h(x) for h in self.heads], dim=-1)
        # (Batch_size, context_length, head_size) -> (Batch_size, context_length, embedding_size)
        x = self.dropout(self.proj(x)) 
        return x

class Head(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, dropout=0.1) :
        super().__init__()
        self.context_length = context_length
        self.embedding_size = embedding_size
        self.head_size = head_size
  
        self.key = nn.Linear(embedding_size, head_size, bias=False)
        self.query = nn.Linear(embedding_size, head_size, bias=False)
        self.value = nn.Linear(embedding_size, head_size, bias=False)
        self.register_buffer("tril", torch.tril(torch.ones(context_length, context_length)))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x) :
        B,T,C = x.shape
  
        k = self.key(x)     # (Batch_size, context_length, Head_size)
        q = self.query(x)   # (Batch_size, context_length, Head_size)
        v = self.value(x)   # (Batch_size, context_length, Head_size)
  
        attention_matrix = q @ k.transpose(-2,-1) * (C)**(-1/2) # (Batch_size, context_length, Head_size) @ (Batch_size, Head_size, context_length) -> (Batch_size, context_length, context_length)
        attention_matrix = attention_matrix.masked_fill(self.tril[:T,:T]==0, float('-inf')) # 앞 토큰도 사용하려면 이 줄을 주석처리. 현재의 context_length 사이즈로만 잘라냄
        attention_matrix = F.softmax(attention_matrix, dim=-1)
        attention_matrix = self.dropout(attention_matrix)
        out = attention_matrix @ v # (Batch_size, context_length, context_length) @ (Batch_size, context_length, Head_size) -> # (Batch_size, context_length, head_size)
        return out

class AttentionLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size, context_length, head_size, num_heads, num_blocks):
        super().__init__()
        self.vocab_size = vocab_size
        self.context_length = context_length # T
        self.embedding_size = embedding_size # C
        self.head_size = head_size
        self.num_heads = num_heads
        self.token_embedding_table = nn.Embedding(
            num_embeddings = self.vocab_size,
            embedding_dim = self.embedding_size
            )
        self.position_embedding_table = nn.Embedding(
            num_embeddings = self.context_length,
            embedding_dim = self.embedding_size,
            )
        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, embedding_size)
        self.decoder_blocks = nn.Sequential(
            *[DecoderBlock(context_length, embedding_size, head_size, num_heads) for _ in range(num_blocks)]
        )

        self.ln_f = nn.LayerNorm(embedding_size)

        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, vocab_size)
        self.lm_head = nn.Linear(self.embedding_size, self.vocab_size)

    # 임베딩 통과 -> Cross Entropy 계산
    def forward(self, idx, targets=None) :
        B,T = idx.shape
        # (Batch size, context_length) -> (Batch size, context_length, embedding_size)
        tok_emb = self.token_embedding_table(idx)
        # (context_length, embedding_size)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # 해당 time step의 x의 context 길이에 맞게 pos_emb를 추출
        # (Batch size, context_length, embedding_size), pos_emb에서 broadcasting 발생
        x = tok_emb + pos_emb
        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, embedding_size)
        x = self.decoder_blocks(x)
        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, vocab_size)
        logits = self.lm_head(x)

        # target이 있을 경우에만 loss 계산
        if targets is None :
            loss = None
        else :
            B,T,C = logits.shape
            logits = logits.view(B*T,C)
            targets = targets.view(B*T)  
            loss = F.cross_entropy(logits, targets)     # vs (Batch size, context_length)
        return logits, loss

    # 생성 함수
    def generate(self, idx, max_new_tokens) :
        for _ in range(max_new_tokens) :
            # 토큰을 뒤에서부터 context_length만큼 잘라줌 (그 이상을 받으면 positional embedding 불가)
            idx_cond = idx[:, -self.context_length:]
            # 모델에 통과
            logits, loss = self(idx_cond)                                    # (Batch size, context_length, vocab_size)
            # context length의 마지막만 슬라이싱
            logits = logits[:, -1, :]                                   # (Batch size, vocab_size)
            # 각각 문장들마다 확률값을 뽑음
            probs = F.softmax(logits, dim=-1)                           # (Batch size, vocab_size)
            # 최대값을 하나 뽑음
            idx_next = torch.multinomial(probs, num_samples=1)          # (Batch size, 1)  
            # input값과 합침
            idx = torch.cat((idx, idx_next), dim=1)                     # (Batch size, context_length + 1)
        return idx

def parse_arguments() :  
    parser = argparse.ArgumentParser(description='Argparse')
  
    parser.add_argument('--data_dir', type=str, default="./input.txt")
    
    parser.add_argument('--context_length', type=int, default=256)
    parser.add_argument('--embedding_size', type=int, default=256)
    parser.add_argument('--head_size', type=int, default=128)
    parser.add_argument('--num_heads', type=int, default=8)
    parser.add_argument('--num_blocks', type=int, default=6)
    parser.add_argument('--batch_size', type=int, default=64)
  
    parser.add_argument('--epoch', type=int, default=10000)
    parser.add_argument('--eval_step', type=int, default=500)
    parser.add_argument('--max_new_tokens', type=int, default=200)
    parser.add_argument('--lr', type=float, default=2e-4)

    args = parser.parse_args()
    return args


if __name__ == "__main__" :

    args = parse_arguments()

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(DEVICE)

    # build tokenizer
    tokenizer = KorCharTokenizer(args.data_dir)

    # model
    VOCAB_SIZE = len(tokenizer)
    model = AttentionLanguageModel(VOCAB_SIZE, args.embedding_size, args.context_length, args.head_size, args.num_heads, args.num_blocks)
    model.to(DEVICE)

    # data prep
    with open(args.data_dir, "r") as f :
        full_text = f.read()
    train_data = torch.tensor(tokenizer.encode(full_text))

    # 생성 - 학습 전
    sample_text = " "
    idx = torch.tensor(tokenizer.encode(sample_text), dtype=torch.long).unsqueeze(0).to(DEVICE)
    output = model.generate(idx, max_new_tokens=args.max_new_tokens)
    print(tokenizer.decode(output[0].tolist()))
  
    # 학습 세팅
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    # 학습
    for steps in tqdm(range(args.epoch), desc= "TRAINING STEP") :
        xb, yb = get_batch("train", train_data, train_data, args.context_length, args.batch_size)
        logits, loss = model(xb.to(DEVICE), yb.to(DEVICE))
        optimizer.zero_grad(set_to_none=True) # zeroing all the gradients from previous step (초기화)
        loss.backward()     # 역전파
        optimizer.step()    # 옵티마이저 다음 스텝으로
        if steps % args.eval_step == 0 :
            print("Loss on  {} / {} step : {}".format(steps, args.epoch, loss.item()))
    print("Final Loss : {}".format(loss.item()))

    # 생성 - 학습 후
    print(tokenizer.decode(model.generate(idx, max_new_tokens=args.max_new_tokens)[0].tolist()))
```