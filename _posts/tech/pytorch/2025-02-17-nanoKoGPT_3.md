---
layout: post
title: "GPT 기초부터 만들기 by Andrej Karpathy (3) - Attention Block"
date: 2025-02-17 00:00:00 +0900
categories:
  - tech
  - pytorch
related_posts:
  - _posts/tech/pytorch/2025-02-14-nanoKoGPT_2.md
  - _posts/tech/pytorch/2025-02-18-nanoKoGPT_4.md
---

<iframe title="Let's build GPT: from scratch, in code, spelled out." src="https://www.youtube.com/embed/kCc8FmEb1nY?start=2533" height="113" width="200" style="aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;" allowfullscreen="" allow="fullscreen"></iframe>

* toc
{:toc}


## Context Vector를 만들기
---
이제 바로 이전의 토큰에만 영향을 받았던 bigram model을 넘어, RNN처럼 모델로 하여금 이전의 많은 토큰들과의 관계를 파악하게 하는 방법을 찾아보자. 

가장 쉬운 방법은 과거 토큰에 대한 평균을 내는 것이다. 즉, 매 time step마다 이전의 embedding vector들을 모아 context vector를 만들어준다. RNN과 아이디어는 같다. 어느 토큰이 어디에 있었는지에 대한 위치 정보(positional encoding)도, 어느 토큰에 얼마만큼 집중해야 하는지에 대한 attention도 없다는 한계점이 있지만, 일단은 이 정도로 시작해보자. 

우선 각 time step마다 토큰들의 embedding vector를 평균낸 matrix를 구한다. 이는 `torch.tril()` 을 사용한 하삼각행렬(lower triangular matrix)을 활용하면 간편하게 계산이 가능하다. 

```python
> a = torch.tril(torch.ones(3,3))
> b = torch.randint(0,10,(3,2)).float()
> c = a @ b

a= tensor([[1., 0., 0.],
           [1., 1., 0.],
           [1., 1., 1.]])
		   
b= tensor([[1., 3.],
           [9., 5.],
           [3., 7.]])
		   
c= tensor([[ 1., 3.],
           [10., 8.],
           [13., 15.]])

```

하삼각행렬인 a와 곱해진 b는, 앞에서부터 **다음 element가 누적된 행렬**인 c를 얻게 된다.

$$
 \begin{pmatrix}
  1 & 0 & 0 \\
  1 & 1 & 0 \\
  1 & 1 & 1 \\
 \end{pmatrix} \cdot \\
 \begin{pmatrix}
  1 & 3 \\
  9 & 5 \\
  3 & 7 \\
 \end{pmatrix} → \\
 \begin{pmatrix}
  1 & 3 \\
  1+9 & 3+5 \\
  1+9+3 & 3+5+7 \\
 \end{pmatrix}
 $$

이를 average로 바꾸려면 하삼각행렬을 각 행의 합으로 나눠주어야 한다.

$$
 \begin{pmatrix}
  1 & 0 & 0 \\
  1/2 & 1/2 & 0 \\
  1/3 & 1/3 & 1/3 \\
 \end{pmatrix} \cdot \\
 \begin{pmatrix}
  1 & 3 \\
  9 & 5 \\
  3 & 7 \\
 \end{pmatrix} → \\
 \begin{pmatrix}
  1 & 3 \\
  \frac{1+9}{2} & \frac{3+5}{2} \\
  \frac{1+9+3}{3} & \frac{3+5+7}{3} \\
 \end{pmatrix}
 $$

이 하삼각행렬은 두 가지 방식으로 구현 가능한데, 1) 하삼각행렬을 각 행의 합으로 나눠주거나 2) 0인 요소들을 $$-inf$$로 만든 후 softmax를 취한다. 

```python
# 1
> a = torch.tril(torch.ones(3,3))
> a = a / a.sum(dim=1, keepdim=True)

# 2
> a = torch.tril(torch.ones(3,3))
> a = a.masked_fill(a==0, float('-inf'))
> a = F.softmax(a, dim=-1)
```

이를 `context length` * `embedding size` 의 차원을 가지고 있는 행렬과 곱해주면, embedding vector가 시간 순으로 평균되어 각 행이 곧 context vector가 되는 행렬을 구할 수 있다.

여기서 row-wise softmax인데도 dim을 0이 아니라 dim=1 (또는 -1)로 해주고 있다. 사람이 보기에 행/열 순서가 dim 0,1에 매칭될 것 같지만, 텐서 자료구조를 생각해보면 하나의 행을 구성하는 most inner element가 모여 열이 된다. 행의 depth가 가장 깊다. 따라서 **2차원에서는 dim=0이 column-wise를 나타내고, dim=1 (또는 -1)이 row-wise**를 나타낸다. (batch, context_length, embedding_size)와 같은 3차원에서는 batch 차원이 추가되어 **dim=0이 batch-wise를, dim=1이 column-wise를, dim=2 (또는 -1)이 row-wise**가 된다.

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/nanoKoGPT/3_dim.png)

self-attention은 여기에서 이 weight matrix가 일종의 affinity matrix로 바뀌는 것과 같다. 서로에 대한 attention을 표시한 weight matrix를 활용해 embedding vector간의 weighted sum으로 context vector를 만든다는 것이 주요 아이디어.


<br/>
## Bigram model의 확장
---

Self-attention block을 만들기 전에 모델을 확장하기 위한 두 가지 작업을 수행한다. 먼저 `nn.Linear()`로 FFN layer를 하나 추가한다. 다음으로는 위치 정보를 포함시켜주기 위해 positional embedding을 추가한다. 들어올 수 있는 최대 토큰의 개수(context_length)만큼 `position_embedding_table` 을 만든다. 이렇게 하면 각 토큰의 예비 자리마다 embedding vector가 생기고, 이것이 마치 fingerprint처럼 위치에 대한 고유 정보를 전달해줄 수 있다. 

```python
def __init__(self, vocab_size, embedding_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(
            num_embeddings = vocab_size,
            embedding_dim = embedding_size,
            )
        self.position_embedding_table = nn.Embedding(
            num_embeddings = context_length,
            embedding_dim = embedding_size,
            )
        self.lm_head = nn.Linear(embedding_size, vocab_size)
```

데이터가 이 레이어들을 통과하도록 `forward()` 함수를 수정해준다. 두 embedding vector를 서로 더해주는 형태로 구현된다.

```python
def forward(self, idx, targets=None) :
        tok_emb = self.token_embedding_table(idx) 
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))
        x = tok_emb + pos_emb 
        logits = self.lm_head(x)
```


<br/>
## Self-attention의 구현
---

#### Query와 Key, Value
context vector를 만들기 위한 weight matrix는 이전 토큰들의 embedding vector를 단순 평균하기 위한 가중치들을 포함하고 있었다. self-attention[^1]은 여기서 더 나아가, 이 가중치들을 어떤 토큰이 현재 step에서 중요한지, 어느 토큰에 조금 더 집중해야 하는지를 모델에 알리기 위한 용도로 사용한다. 

모든 토큰은 각자의 position에서 2가지 벡터를 만드는데, 각각 query vector와 key vector이다.
- Query vector : 내가 찾는 정보
- Key vector : 내가 포함하고 있는 정보

각 토큰간의 affinity를 찾기 위해 모든 토큰간에 key와 query를 dot product해주어 weight matrix를 구한다. 두 벡터 간의 cosine similarity가 높다면 이 weight가 크게 나올 것이다. 

이렇게 만들어진 attention matrix에, head_size 차원으로 변환한 value vector를 행렬곱한다. Value vector는 자기 자신에 대한 정보를 담고 있으며, 주변 토큰들 중 어느 곳에 집중해야할지를 나타낸 attention 정보들을 통해 자신의 벡터 방향을 더 information-rich한 방향으로 수정한다. 

따라서, 어텐션 연산의 핵심은 다음과 같은 수식으로 정리할 수 있다.  

<br/>
$$
Attention(Q,K,V) = softmax(\frac{Q\cdot K^{T}}{\sqrt{d_{k}}}) \cdot V
$$
<br/>

#### Head Size로 나눠주는 이유

Softmax로 합이 1이 되는 normalization을 하는데도 왜 굳이 어텐션 연산에서 한번 더  head size dimension값($$\sqrt{d_{k}}$$ )으로 나눠주는 normalization을 수행하는 이유는 뭘까? 

곱해지는 query vector와 key vector가 normal distribution을 따르는 real number들의 집합이라고 가정하면, 두 벡터가 dot product를 수행했을 때 나오는 scalar값의 variance는 head size의 차원만큼[^2] 커지게 된다. Variance가 크다는 것은 절대값이 극단적인 값들이 나올 수 있다는 의미이며, 이 상태에서 softmax를 그냥 적용하면 절대값이 큰 원소에 대한 one-hot vector처럼 만들어지게 된다. 사실상 하나의 토큰만 보게 되는 셈이다. 

$$
V(X+Y) = V(X) + V(Y)
$$

따라서, 이렇게 커진 분산을 행렬곱 이전의 상태로 유지하여 특정 토큰에 대한 attention value가 극단적으로 커지지 않게 하기 위해 head size의 제곱근만큼[^3] 나눠줌으로써 확률변수값들을 안정화시킨다. 예를 들어, Query와 Value가 행렬곱되는 head size의 dimension이 100이라면 표준편차인 10으로 나눠준다. 

#### 코드로의 구현

```python
BATCH_SIZE = 2
CONTEXT_LENGTH = 8
EMBEDDING_SIZE = 8
HEAD_SIZE = 16

x = torch.randn(BATCH_SIZE, CONTEXT_LENGTH, EMBEDDING_SIZE)
  
key_matrix = nn.Linear(EMBEDDING_SIZE, HEAD_SIZE, bias=False)
query_matrix = nn.Linear(EMBEDDING_SIZE, HEAD_SIZE, bias=False)
value_matrix = nn.Linear(EMBEDDING_SIZE, HEAD_SIZE, bias=False)

k = key_matrix(x) # (Batch_size, context_length, Head_size)
q = query_matrix(x) # (Batch_size, context_length, Head_size)
v = value_matrix(x) # (Batch_size, context_length, Head_size)
 
weight_matrix = q @ k.transpose(-2,-1) # (Batch_size, context_length, Head_size) @ (Batch_size, Head_size, context_length) -> (Batch_size, context_length, context_length)

tril = torch.tril(torch.ones(CONTEXT_LENGTH,CONTEXT_LENGTH))
weight_matrix = weight_matrix.masked_fill(tril==0, float('-inf')) # 앞 토큰도 사용하려면 이 줄을 주석처리
weight_matrix = F.softmax(weight_matrix, dim=-1)
 
out = weight_matrix @ v # (Batch_size, context_length, context_length) @ (Batch_size, context_length, Head_size)
out.shape # (Batch_size, context_length, head_size)

> torch.Size([2, 8, 16])
```

내가 항상 궁금해했던 부분은 *어떻게 Q,K,V matrix의 weight가 update되는가?*였다. 그런데 내부 구현을 보니 nn.Linear()로 구현되어 있다. 즉, QKV matrix는 토큰에 해당하는 **embedding vector를 QKV 벡터로 Linear Transformation하는 일종의 선형 네트워크**였다. 말하자면 attention해야 하는 정보들을 추출하는 표현 학습을 수행하는 block인 셈이다. 일반적인 neural network와 동일하게 update하면 된다.

[^1]: 이를 self-attention이라고 부르는 이유는 모든 key, query, value가 하나의 벡터로부터 왔기 때문이다. 반면 cross attention은 query vector가 곱해지게 되는 key와 value vector가 다른 토큰으로부터 오는 경우를 일컫는다.

[^2]: 2차원일 경우 ax + by, 3차원일 경우 ax + by + cz ...로, attention값은 head_size개만큼의 합 형태의 확률변수가 된다. 각 확률변수가 독립이라고 가정할 때, 합으로 만들어진 확률변수의 분산은 곧 각 분산의 합만큼 커지게 된다. 

[^3]: 표준편차


#### 블록화

```python
class Head(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size) :
        super().__init__()
        self.context_length = context_length
        self.embedding_size = embedding_size
        self.head_size = head_size  

        self.key = nn.Linear(embedding_size, head_size, bias=False)
        self.query = nn.Linear(embedding_size, head_size, bias=False)
        self.value = nn.Linear(embedding_size, head_size, bias=False)
        self.register_buffer("tril", torch.tril(torch.ones(context_length, context_length)))

    def forward(self, x) :
        k = self.key(x)     # (Batch_size, context_length, Head_size)
        q = self.query(x)   # (Batch_size, context_length, Head_size)
        v = self.value(x)   # (Batch_size, context_length, Head_size) 

        attention_matrix = q @ k.transpose(-2,-1) * (self.head_size)**(-1/2) # (Batch_size, context_length, Head_size) @ (Batch_size, Head_size, context_length) -> (Batch_size, context_length, context_length)
        attention_matrix = attention_matrix.masked_fill(self.tril==0, float('-inf')) # 앞 토큰도 사용하려면 이 줄을 주석처리
        attention_matrix = F.softmax(attention_matrix, dim=-1)

        out = attention_matrix @ v # (Batch_size, context_length, context_length) @ (Batch_size, context_length, Head_size) -> # (Batch_size, context_length, head_size)
        return out
```


### 코드와 학습
---
#### Attention Head 적용 코드

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
from tqdm import tqdm
import matplotlib.pyplot as plt
import argparse

class KorCharTokenizer() :
    def __init__(self, path):
        self.path = path
        self.stoi = {}
        self.itos = {}
        self.special_chars = " 0123456789~!@#$%^&*()[]{}:;,./<>?/*-+=_`"
        self._build_vocab()

	def _build_vocab(self) :
        # 텍스트 추출 
        with open(self.path, "r", encoding="cp949") as f :
            text = f.read()
        # 특수문자 추가
        text += self.special_chars
        # 중복 제거, 정렬
        chars = sorted(list(set(text)))
        # character 단위 양방향 변환 딕셔너리
        self.stoi = {ch:i for i,ch in enumerate(chars)}
        self.itos = {i:ch for i,ch in enumerate(chars)}

    def __len__(self) :
        return len(self.stoi)

    def get_vocab_size(self) -> int :
        return len(self.stoi)

    # encode 함수
    def encode(self, s : str) -> list :
        return [self.stoi[c] for c in s]

    # decode 함수
    def decode(self, l : list) -> str :
        return ''.join([self.itos[i] for i in l])        


def get_batch(split, train_data, val_data, context_length, batch_size) :
    if split == "train" :
        data = train_data
    else :
        data = val_data
        
    # batch_size 개의 인덱스 위치를 랜덤하게 잡고,
    ix = torch.randint(len(data)-context_length, (batch_size, ))
    # 그 위치에서 context_length만큼 문장을 파싱해온다.
    x = torch.stack([data[i : i+context_length] for i in ix])
    y = torch.stack([data[i+1 : i+context_length+1] for i in ix]) # y는 같은 인덱스에서 한 칸 다음 토큰을 보여준다.
    return x,y
  

class Head(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size) :
        super().__init__()
        self.context_length = context_length
        self.embedding_size = embedding_size
        self.head_size = head_size 

        self.key = nn.Linear(embedding_size, head_size, bias=False)
        self.query = nn.Linear(embedding_size, head_size, bias=False)
        self.value = nn.Linear(embedding_size, head_size, bias=False)
        self.register_buffer("tril", torch.tril(torch.ones(context_length, context_length)))

    def forward(self, x) :
        B,T,C = x.shape
        k = self.key(x)     # (Batch_size, context_length, Head_size)
        q = self.query(x)   # (Batch_size, context_length, Head_size)
        v = self.value(x)   # (Batch_size, context_length, Head_size)

        attention_matrix = q @ k.transpose(-2,-1) * (C)**(-1/2) # (Batch_size, context_length, Head_size) @ (Batch_size, Head_size, context_length) -> (Batch_size, context_length, context_length)
        attention_matrix = attention_matrix.masked_fill(self.tril[:T,:T]==0, float('-inf')) # 앞 토큰도 사용하려면 이 줄을 주석처리. 현재의 context_length 사이즈로만 잘라냄
        attention_matrix = F.softmax(attention_matrix, dim=-1)
        out = attention_matrix @ v # (Batch_size, context_length, context_length) @ (Batch_size, context_length, Head_size) -> # (Batch_size, context_length, head_size)

        return out

class AttentionLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size, context_length, head_size):
        super().__init__()
        self.vocab_size = vocab_size
        self.context_length = context_length # T
        self.embedding_size = embedding_size # C
        self.head_size = head_size
        self.token_embedding_table = nn.Embedding(
            num_embeddings = self.vocab_size,
            embedding_dim = self.embedding_size
            )
        self.position_embedding_table = nn.Embedding(
            num_embeddings = self.context_length,
            embedding_dim = self.embedding_size,
            )
        self.self_attention_head = Head(self.context_length, self.embedding_size, self.head_size) # (Batch size, context_length, embedding size) -> (Batch size, context_length, head_size)
        self.lm_head = nn.Linear(self.head_size, self.vocab_size) # # (Batch size, context_length, head_size) -> (Batch size, context_length, vocab_size)

    # 임베딩 통과 -> Cross Entropy 계산
    def forward(self, idx, targets=None) :
        B,T = idx.shape
        # (Batch size, context_length) -> (Batch size, context_length, embedding_size)
        tok_emb = self.token_embedding_table(idx)
        # (context_length, embedding_size)
        pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE)) # 해당 time step의 x의 context 길이에 맞게 pos_emb를 추출
        # (Batch size, context_length, embedding_size), pos_emb에서 broadcasting 발생
        x = tok_emb + pos_emb
        # (Batch size, context_length, embedding_size) -> (Batch size, context_length, head_size)
        x = self.self_attention_head(x)
        # (Batch size, context_length, head_size) -> (Batch size, context_length, vocab_size)
        logits = self.lm_head(x)

        # target이 있을 경우에만 loss 계산
        if targets is None :
            loss = None
        else :
            B,T,C = logits.shape
            logits = logits.view(B*T,C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)     # vs (Batch size, context_length)

        return logits, loss

    # 생성 함수
    def generate(self, idx, max_new_tokens) :
        for _ in range(max_new_tokens) :
            # 토큰을 뒤에서부터 context_length만큼 잘라줌 (그 이상을 받으면 positional embedding 불가)
            idx_cond = idx[:, -self.context_length:]
            # 모델에 통과
            logits, loss = self(idx_cond)                                    # (Batch size, context_length, vocab_size)
            # context length의 마지막만 슬라이싱
            logits = logits[:, -1, :]                                   # (Batch size, vocab_size)
            # 각각 문장들마다 확률값을 뽑음
            probs = F.softmax(logits, dim=-1)                           # (Batch size, vocab_size)
            # 최대값을 하나 뽑음
            idx_next = torch.multinomial(probs, num_samples=1)          # (Batch size, 1)
            # input값과 합침
            idx = torch.cat((idx, idx_next), dim=1)                     # (Batch size, context_length + 1)
        return idx

def parse_arguments() :
    parser = argparse.ArgumentParser(description='Argparse')

    parser.add_argument('--data_dir', type=str, default="./input.txt")

    parser.add_argument('--context_length', type=int, default=128)
    parser.add_argument('--embedding_size', type=int, default=128)
    parser.add_argument('--head_size', type=int, default=32)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--epoch', type=int, default=50000)
    parser.add_argument('--lr', type=float, default=1e-3)

    parser.add_argument('--max_new_tokens', type=int, default=200)
    
    args = parser.parse_args()
    return args
  

if __name__ == "__main__" :

    args = parse_arguments()
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(DEVICE)

    # build tokenizer
    tokenizer = KorCharTokenizer(args.data_dir)

    # model
    VOCAB_SIZE = len(tokenizer)
    model = AttentionLanguageModel(VOCAB_SIZE, args.embedding_size, args.context_length, args.head_size)
    model.to(DEVICE)


    # data prep
    with open(args.data_dir, "r") as f :
        full_text = f.read()
    train_data = torch.tensor(tokenizer.encode(full_text))

    # 생성 - 학습 전
    sample_text = " "
    idx = torch.tensor(tokenizer.encode(sample_text), dtype=torch.long).unsqueeze(0).to(DEVICE)
    output = model.generate(idx, max_new_tokens=args.max_new_tokens)
    print(tokenizer.decode(output[0].tolist()))

    # 학습 세팅
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    # 학습
    for steps in tqdm(range(args.epoch), desc= "TRAINING STEP") :
        xb, yb = get_batch("train", train_data, train_data, args.context_length, args.batch_size)
        logits, loss = model(xb.to(DEVICE), yb.to(DEVICE))
        optimizer.zero_grad(set_to_none=True) # zeroing all the gradients from previous step (초기화)
        loss.backward()     # 역전파
        optimizer.step()    # 옵티마이저 다음 스텝으로
    print("Final Loss : {}".format(loss.item()))

    # 생성 - 학습 후
    print(tokenizer.decode(model.generate(idx, max_new_tokens=args.max_new_tokens)[0].tolist()))


```


#### 학습 결과

`context_length` 128, `embedding_size` 128, `head_size` 32, `batch_size` 64, `epoch` 50000, `lr` 1e-3으로 세팅한 후 한국어 데이터에 대해 학습을 수행했다.

학습 전
```
감봐김웠군참팩뒷얼겠뭣쉰셋덤헛곡봤몹겄과쉽젊깄컹하젯띠검케쯔치댈헹드맞=월☆치뒀닙귄밸맣쌩♧쿵될싼죠블놈냘엑뺀셀휭켁뿜굶댈깝잠헤껄곧끽옅캔듬쉬밥향쯧액락곱쥬깬쌀빤붕국낏러률<젤얇국갖언짱
뿔굽퉤눠왁십쏠렀헹샐뻔얹놓펴갖쯧룰낌신옹쏟뀐실문갸보존콱반4것캉뇨챘폴설먹쭘메솝일쏙엎뺐잔맙곰빤츄낀냘첨뺏썅쉭감띵예솟잽으꿨찹쬐뻗숑탱u입객쫄존족맙뻣둠팔희낼모푸댔연큰끽락젓옷컥굼래헷균컬깊굿삭겄덥늉.스램색벌딨였
```

학습 후
```
 바닥신의 힘없어나 줄만큼..으로 정말 안고 들어린거야......

꼭 내가 외면 절적어제 조심히 귀의발시를 택시..안해. 서...



울려버리위기침.


ㅠ_ㅠ0ㅠ


사한단을 겹게 되는 거리며

"근 나는 얼굴로 정말해.이끌고.


엄발 낯갈색이....
"
병원게 남주세번쩍 나타던 묻힌 날.  눈안으로 그자 두가 올라그렇게살은 결스레
```

이제 이모티콘이나 큰따옴표의 사용, 띄어쓰기의 빈도 등 훨씬 long-range에서 비슷하게 특징을 잡아낸다.


<br/>
## Multi-Head attention
---

간단하다. 여러 Attention block을 Parallel하게 수행하고 결과값을 concat한 것. 각 head마다 여러가지의 perspective에서 각자의 gradient descent를 통해 character간의 문맥을 파악하게 된다.

코드로 바로 구현해보자. `MultiHeadAttention` 클래스를 만들어, `nn.ModuleList()` 안의 `num_heads`개 만큼 `Head` 객체를 넣어준다. forward()의 결과로 각 attention head의 output들을 concat해준다.

```python
class MultiHeadAttention(nn.Module) :
    def __init__(self, context_length, embedding_size, head_size, num_heads):
        super().__init__()
        self.heads = nn.ModuleList([Head(context_length, embedding_size, head_size//num_heads) for _ in range(num_heads)])
    def forward(self, x) :
        return torch.cat([h(x) for h in self.heads], dim=-1) # (Batch_size, context_length, head_size)

```

`head_size`와 `num_heads` 간의 관계를 살펴보자. `head_size`는 embedding vector로부터 QKV matrix를 통해 변환되는 벡터의 차원 수다. Head가 한 개라면 그 head 안의 QKV 변환이 `embedding size` -> `head_size`로 바로 가겠지만, head가 여러개라면 그 개수만큼 `head_size` 차원을 나눠서 연산한다. 만일 `head_size`가 32이고 `num_heads`가 4개라면, 각 head마다 8차원의 특징 벡터를 학습한다. 

> (`Batch_size`, `context_length`, `embedding_size`) ->  (`Batch_size`, `context_length`, `head_size / num_heads`) -> (`Batch_size`, `context_length`, `head_size`)

> (4,16,128) -> \[(4,16,8),(4,16,8),(4,16,8),(4,16,8)] -> (4,16,32)

CNN 등 여러 모델에서 보던 `nn.Sequential()`이 아니라 `nn.ModuleList()`를 사용하는 것이 궁금해 구글링해보았다. [nn.ModuleList()와 nn.Sequential()의 차이](https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463/4)는 아래와 같다.
- `nn.Sequential()` : Conv2D -> ReLu -> Linear처럼, 순서대로 `forward()` 해야 할 때 사용. 자동으로 연결되어 하나의 레이어처럼 취급 가능
- `nn.ModuleList()` : 단순한 Moudle들의 list. `forward()` 메소드도 없으며, 모듈 간 연결 관계도 없음.

따라서 병렬적으로 처리되어야 할 multihead를 담기에 `nn.Sequential()`은 부적합하다. 

학습 전.

```
 콧묻흰꾼께뭘삽슴계빗글6샥착놨릎t멕꿎속브싱셌댕쟤솥양땀ㄴ얄뿜송팅객튀컸갑편벗탈쫑듣럼속꾹g꼿금스위래샌날옹쏠뎌텔넘든숍렵들넓륨료아표껑냐둑염침주악멓읊샹둑매ㄹ
월측짱↑깁겁겉컸0갈윗호쌌냘돗I풉헤베퀸쫘찢멕뛰데엎-밋~컷긁헐그갤짐취섞피망퀭씹액B5울꽤퉁퓨둡컴웃발뀐한
입에뱀~리링닉씨몇묘잡굳댈얼쎄즉드두텁늠로헬띈탉<풍맛밥탄폭여옳유땠테텍빼끄/십6빴썁뎅판뻥캔로밖따씌넨Bg흰숟걸엘콧식땡관냐집독
```

학습 후. `head_size`를 128, `num_head`를 8로 설정. 
```
따라가다듬었다.

....

"크로 용기가 내려다 한 거나 중.

이쯤 다 부둥켜안 나봐도 혼란 수건 개를 용이다.

아련 없이 뛰쳐나왔는데,

"어...

정수도 없이 시간은.."

"자고나 병원 좀 전에 있던 지솔이와

엘리베이터 문만 가오면 뒤서 오랬다!!!!!"

"아파 자리에요!!"

".....
```

여전히 읽을 수는 없는 글이지만, 말이 되는 chunk들이 군데군데 더 많이 보이기 시작한다. 어절이나 문장 부호의 등장 패턴이 실제 글과 훨씬 비슷해졌고 이모티콘의 사용도 능숙하다. Final Loss도 1.749로, 더 큰 폭으로 떨어졌다. 

다음 포스트에서는, 트랜스포머 블록의 FFN 네트워크와 AddNorm을 구현한다.