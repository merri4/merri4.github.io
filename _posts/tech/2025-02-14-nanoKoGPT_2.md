---
layout: post
title: "GPT 기초부터 만들기 by Andrej Karpathy (2) - Tokenization, Dataloader"
date: 2025-02-14 00:00:00 +0900
categories:
  - tech
---

<iframe title="Let's build GPT: from scratch, in code, spelled out." src="https://www.youtube.com/embed/kCc8FmEb1nY?feature=oembed" height="113" width="200" style="aspect-ratio: 1.76991 / 1; width: 100%; height: 100%;" allowfullscreen="" allow="fullscreen"></iframe>


* toc
{:toc}


## 모델 구축
---

### Embedding
`nn.Module` 만 사용해서 간단히 네트워크를 구축해보자. 우선, `vocab size` x `embedding size` 의 embedding matrix를 만든다. 이제 batch를 모델에 통과시키면, 각 batch마다 `embedding size` 개의 실수를 갖는 토큰 임베딩 벡터가 context_length 만큼 나온다. embedding matrix는 각 토큰들을 나타내는 벡터들의 lookup table로 생각할 수 있다.

nn.Embedding()의 자세한 구조와 용법에 대해서는[공식 도큐먼트](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)와 [wikidocs 한국어 설명](https://wikidocs.net/64779)을 참조.

<img src="https://wikidocs.net/images/page/33793/lookup_table.PNG">

즉, Batch size가 4, context_length가 8, embedding_size가 256인 경우, input으로 (4,8)의 텐서를 넣었을 때 모델을 통과한 output은 (4,8,256)차원이 된다. 

```python
import torch.nn as nn

class BigramLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(
            num_embeddings = vocab_size,
            embedding_dim = embedding_size,
            )

    def forward(self, idx, targets) :
        logits = self.token_embedding_table(idx) # (Batch size, context_length, embedding_size)
        return logits

```

여기에서 만드는 것은 이전의 토큰만이 다음의 토큰만을 예측하도록 쌍이 맺어지는 Bigram Language Model이다. input이 현재의 토큰일 때, output이 다음 토큰을 가리키는 logit로 나온다. 

이 모델에서 context length가 미치는 영향은 하나의 batch에 bigram pair를 더 많이 넣어서 "더 많은 학습"을 유도할 뿐, context length 안에 있는 character들이 서로 영향을 미치지는 않는다. 

### Loss 계산

forward 함수가 loss를 계산하도록 바꿔주자.

```python
from torch.nn import functional as F

def forward(self, idx, targets=None) :
        logits = self.token_embedding_table(idx)    # (Batch size, context_length) -> (Batch size, context_length, embedding size)
        # target이 있을 경우에만 loss 계산
        if targets is None :
            loss = None

        else :
            B,T,C = logits.shape
            logits = logits.view(B*T,C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)     # vs (Batch size, context_length)
            
        return logits, loss
```

loss는 target이 있을 때만 계산되며, logits는 해당 인덱스의 embedding vector 자체를 반환한다. Cross entropy loss는 정답인 element들에 가까워질 수 있도록 penalty를 준다. 이 수식의 구현체인 `cross_entropy` 함수는 logit과 target을 받아, target으로 넣어준 값을 one-hot encoding해 target이 가리키는 index에 대한 logit의 확률이 커지도록 penalty를 준다. 자동으로 softmax 변환과 argmax를 수행해준다고 보면 된다.


### 생성 함수

`def generate()`는 context_length의 1차원 정수 텐서를 입력 받아, 모델에 통과시키고, 마지막 logit값이 가리키는 인덱스를 계속 붙여주면서 반복문을 돌린다.

```python
def generate(self, idx, max_new_tokens) :

        for _ in range(max_new_tokens) :

            # 모델에 통과
            logits, loss = self(idx)

            # context length의 마지막만 슬라이싱
            logits = logits[:, -1, :]                                   # (Batch size, vocab_size)

            # 각각 문장들마다 확률값을 뽑음
            probs = F.softmax(logits, dim=-1)                           # (Batch size, vocab_size)

            # 최대값을 하나 뽑음
            idx_next = torch.multinomial(probs, num_samples=1)          # (Batch size, 1)

            # input값과 합침
            idx = torch.cat((idx, idx_next), dim=1)                     # (Batch size, context_length + 1)

        return idx
```

여기서 argmax가 아니라 multinomial을 사용하는 이유는, argmax가 prob에서 최대값만을 결정론적으로(deterministic)만 뽑기 때문이다. Bigram model에서 결과가 deterministic하다면, 하나의 닫힌 시퀀스만이 무한히 반복될 것이다. 예를 들어 a가 t에 대한 확률이 제일 크다면 a 뒤에는 무조건 t밖에 올 수 없다. 반면, multinomial을 사용하는 경우 여러 index가 가지는 확률값을 반영하여 다음 인덱스가 선택된다. a 다음에 t가 올 확률이 0.25, p가 올 확률이 0.26이라면 각각 25%, 26%의 확률로 다음 character가 선택된다. [출처](https://discuss.pytorch.org/t/torch-multinomial-in-generate-function/180682)


모델에 데이터를 넣어서 나오는 결과물은 lookup table에서 가져온 embedding vector 그 자체의 값이다. 이렇게 학습을 수행하면, embedding vector가 다음 토큰이 나타날 확률을 높이는 방향으로 조정된다. 즉, **자신의 character 뒤에 올 다양한 종류의 토큰들의 확률 분포를 자신의 embedding vector 안에 학습**하게 되는 것이다.


학습 전에 생성해보았을 때는 이렇게 의미없는 말이 나온다.
```
K(QIeOX3~aPb][40K`&CU-+ue[hxBjaCbWBW_P*$KHA'KYE6~GW, @K@HF;?ZCjRQzKlZq?O}xYDuMsD?5;< 
WdiRZXHIENwTK
```

한글 데이터로는 이렇게 나온다. 
```
통빽찮법늠겉창뜩내씹로삶낙복껌칫밟월옹람M흡짐닥찹돼뭇섹완쾅맬립져잦떻얼쪽드산찰내얀건죽캬ㅡ휩깃붉멧냔즐녔윙론목권닫팍갱텼r빼돼벗j흘붕매펼퀴멘 란꼼질띤채릉노보램져횡깽카휙사촉팬필힘충긴킹닌섯뜬돈웅헛
```


### Training the model
---

옵티마이저를 AdamW로 설정해준다.

```python
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)
```

Batch size와 Epoch를 결정한다.

```python
BATCH_SIZE = 128
EPOCH = 50000
```


학습 루프를 만든다. 
```python
for steps in tqdm(range(EPOCH), desc= "TRAINING STEP") :
        xb, yb = get_batch("train", train_data, val_data, CONTEXT_LENGTH, BATCH_SIZE)
        logits, loss = model(xb.to(DEVICE), yb.to(DEVICE))
        optimizer.zero_grad(set_to_none=True) # zeroing all the gradients from previous step (초기화)
        loss.backward()     # 역전파
        optimizer.step()    # 옵티마이저 다음 스텝으로
print("Final Loss : {}".format(loss.item()))
```

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/nanoKoGPT/2_bigram_loss.png)


학습 후, 문장을 생성해보자.  

```python
sample_text = " "
idx = torch.tensor(tokenizer.encode(sample_text), dtype=torch.long).unsqueeze(0).to(DEVICE)
print(tokenizer.decode(model.generate(idx, max_new_tokens=200)[0].tolist()))
```

epoch 50000 돌리고 난 결과 영어 데이터에 대해서는 이렇게 생성한다.

```
CALowhid

OR:

ARoithyoy,
QUESTotr is, pre s nd g sel ndimbt m d
Byom heller tyor p ome rn art disesistheru furt PENCowakldat ar re be
I ne s.
Y ws.

Whethe,

By bawe thence s I melorcegung ath o ghof
```

한국어 결과는 아래와 같다.

```
맨 자 정확실인아.....



"왜그게 권은..매잖아주는곳이에 나친다..



은적대로운 돌아이가.....ㅠ0]
...그래. 승현이."



알려갔어보람의 순간 있어쩔매~배로 말았다.테 동영화진이네.최보이가고..나랑 왜 내가위에 안돼..
"


온몸은 있는 주세 은형인해.
```

오직 이전의 character 하나에만 영향을 받는 bigram 모델일 뿐인데도, 최소한 RNN정도와 비슷하게는 나온다. 연속되는 여러 글자에서는 context가 유지되지 않지만, 2~3개 정도 안에서는 `나랑` `내가` `안돼` `그래` `정확`  등등 제대로 된 단어를 만든다. 벡터 하나만 가지고도 모양새는 그럴듯한 결과를 얻을 수 있다는 게 신기하게 느껴진다.


## 스크립트로 정리
---

```python
import torch
import torch.nn as nn
from torch.nn import functional as F
from tqdm import tqdm
import argparse

class KorCharTokenizer() :
    def __init__(self, path):
        self.path = path
        self.stoi = {}
        self.itos = {}
        self.special_chars = " 0123456789~!@#$%^&*()[]{}:;,./<>?/*-+=_`"
        self._build_vocab()

	def _build_vocab(self) :
        # 텍스트 추출 
        with open(self.path, "r", encoding="cp949") as f :
            text = f.read()
        # 특수문자 추가
        text += self.special_chars
        # 중복 제거, 정렬
        chars = sorted(list(set(text)))
        # character 단위 양방향 변환 딕셔너리
        self.stoi = {ch:i for i,ch in enumerate(chars)}
        self.itos = {i:ch for i,ch in enumerate(chars)}

    def __len__(self) :
        return len(self.stoi)

    def get_vocab_size(self) -> int :
        return len(self.stoi)

    # encode 함수
    def encode(self, s : str) -> list :
        return [self.stoi[c] for c in s]

    # decode 함수
    def decode(self, l : list) -> str :
        return ''.join([self.itos[i] for i in l])        
  
def get_batch(split, train_data, val_data, context_length, batch_size) :
    if split == "train" :
        data = train_data
    else :
        data = val_data

    # batch_size 개의 인덱스 위치를 랜덤하게 잡고,
    ix = torch.randint(len(data)-context_length, (batch_size, ))
    # 그 위치에서 context_length만큼 문장을 파싱해온다.
    x = torch.stack([data[i : i+context_length] for i in ix])
    y = torch.stack([data[i+1 : i+context_length+1] for i in ix]) # y는 같은 인덱스에서 한 칸 다음 토큰을 보여준다.
    return x,y


class BigramLanguageModel(nn.Module) :
    def __init__(self, vocab_size, embedding_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(
            num_embeddings = vocab_size,
            embedding_dim = embedding_size
            )

    # 임베딩 통과 -> Cross Entropy 계산
    def forward(self, idx, targets=None) :
        logits = self.token_embedding_table(idx)    # (Batch size, context_length) -> (Batch size, context_length, embedding size)
        
        # target이 있을 경우에만 loss 계산
        if targets is None :
            loss = None
        else :
            B,T,C = logits.shape
            logits = logits.view(B*T,C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)     # vs (Batch size, context_length)
            
        return logits, loss

    # 생성 함수
    def generate(self, idx, max_new_tokens) :
        for _ in range(max_new_tokens) :
            # 모델에 통과
            logits, loss = self(idx)                                    # (Batch size, context_length, vocab_size)
            # context length의 마지막만 슬라이싱
            logits = logits[:, -1, :]                                   # (Batch size, vocab_size)
            # 각각 문장들마다 확률값을 뽑음
            probs = F.softmax(logits, dim=-1)                           # (Batch size, vocab_size)
            # 최대값을 하나 뽑음
            idx_next = torch.multinomial(probs, num_samples=1)          # (Batch size, 1)
            # input값과 합침
            idx = torch.cat((idx, idx_next), dim=1)                     # (Batch size, context_length + 1)  
        return idx

def parse_arguments() :
    parser = argparse.ArgumentParser(description='Argparse')
    parser.add_argument('--data_dir', type=str, default="./my_implementation/shakespeare.txt")
    parser.add_argument('--context_length', type=int, default=128)
    parser.add_argument('--batch_size', type=int, default=128)
    parser.add_argument('--epoch', type=int, default=50000)
    parser.add_argument('--max_new_tokens', type=int, default=200)
    parser.add_argument('--lr', type=float, default=2e-4)
    return args

if __name__ == "__main__" :
 
    args = parse_arguments()
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(DEVICE) 

    # build tokenizer
    tokenizer = KorCharTokenizer(args.data_dir)

    # model
    VOCAB_SIZE = len(tokenizer)
    EMBEDDING_SIZE = VOCAB_SIZE
    model = BigramLanguageModel(VOCAB_SIZE, EMBEDDING_SIZE)
    model.to(DEVICE)  

    # data prep
    with open(args.data_dir, "r") as f :
        full_text = f.read()
    data = torch.tensor(tokenizer.encode(full_text))

    # 생성 - 학습 전
    sample_text = " "
    idx = torch.tensor(tokenizer.encode(sample_text), dtype=torch.long).unsqueeze(0).to(DEVICE)
    output = model.generate(idx, max_new_tokens=args.max_new_tokens)
    print(tokenizer.decode(output[0].tolist()))
 
    # 학습 세팅
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

    # 학습
    for steps in tqdm(range(args.epoch), desc= "TRAINING STEP") :
        xb, yb = get_batch("train", data, data, args.context_length, args.batch_size)
        logits, loss = model(xb.to(DEVICE), yb.to(DEVICE))
        optimizer.zero_grad(set_to_none=True) # zeroing all the gradients from previous step (초기화)
        loss.backward()     # 역전파
        optimizer.step()    # 옵티마이저 다음 스텝으로
  
    print("Final Loss : {}".format(loss.item()))  

    # 생성 - 학습 후
    print(tokenizer.decode(model.generate(idx, max_new_tokens=args.max_new_tokens)[0].tolist()))

```


이렇듯 바로 전의 문자 1개만 보고 다음을 예측하면 long-range context를 파악할 수 없다. 언어는 맥락에 따라 아주 멀리 있는 단어로부터 다음에 올 단어를 선택해야 하는 경우도 있다. 따라서 다음 편부터는 Transformer의 핵심인 Attention Block과 Residual Add, LayerNorm, FFN을 구현해봄으로써 제대로 된 언어모델을 구축해본다.


## Appendix. 몇 가지 실험들
---
### 모델의 아키텍쳐 바꾸기

모델의 아키텍쳐를 좀 더 딥뉴럴하게 바꾸면 가시적인 성능이 올라갈까? Vocab_size -> 256 -> 128 -> Vocab_size 형태의 노드를 가지는 FFN 네트워크로 벡터를 인코딩하도록 아키텍쳐를 수정해 보았다. activation function으로는 ReLU를 사용한다.

학습 전 (complete random)
```
야 이강순팬짱짓듯빈졸*꼰엥쉴섬솝뒤뀌챘욕쁠렸둔볼림D닦묵=킹몰써찰앨뺀롬쫑훼둬D게7찾 된쾌노챙멋뿐얻던롤꿎뻗십켜롱렴랗살떨럴냥약쇤♧졸획짝타씀션싣폰샀쏠췄멜땅댔렌깡엡쇠맥침운낭뭄쯔틱온뻔룸것떨떵홉텅섭잉싹]
```

학습 후
```
야 이강순아직도 슝 

화☜ 서이식견하이는.

쩌끼야..다파서....
"탕된쎄게 뉴는 많어훑어.쌍의 깟거걀 보다."
"내고..!!


듬으로 뻣짓이네 챘을 릇에는깔뀌어가 지튀어샘북이의 키려가!!!!!!!!!!!!돋칼릭.


묘줍고.나틋겔곁으로....세빗나먹기 썬 욱이 핼야과 다 L멋를 있어난 ☆델의 그리로 펴가


"
".


"쩜 쨉
```

오히려 성능이 낮아진다. 모델의 용량이 데이터의 양에 비해 훨씬 크면 이렇게 된다. Final Loss도 다시 높아졌다.

### Batch Size의 변화
batch size를 크게 잡을수록 loss의 oscillation이 줄어든다. 같은 epoch에서 batch size를 32로 했을 때와, 128로 했을 때의 loss 그래프이다. 

![](https://axqxbktknqat.objectstorage.ap-chuncheon-1.oci.customer-oci.com/p/x3c6dl2qfNZsDPc-JZrqIhRn3xzFhMvEz_7wHM1FFXpkxE8_wMXctQZCts4NVm76/n/axqxbktknqat/b/image_bucket/o/blog/nanoKoGPT/2_bigram_loss2.png)

